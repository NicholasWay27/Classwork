---
title: "STAT 400 HW 2"
author: 'Nicholas Way'
date: "Due: Saturday, September 16 by 11:59 PM"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Front Matter
```{r, message=F, warning=F}
rm(list = ls())
library(dplyr)
library(ggplot2)
#Add libraries if needed
```



1. (Interaction) Recall the [gender discrimination study](https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#guided-exercises) and data from HW 1:

```{r, indent = "     "}
banksalary = read.csv(file='https://raw.githubusercontent.com/proback/BeyondMLR/master/data/banksalary.csv', header=T)
```

a. Suppose we are interested in investigating the relationships between beginning salary, experience, and age. Rather than using the employee's age directly, you should crudely classify an employee as "young" or "old" by using the median age as a breakpoint for categorizing workers. (If any employee's age matches the median, assign them to the "young" category.) Then, generate an appropriate color-coded scatterplot with trend lines and comment on whether you think there is an interaction effect between experience and age group. 

```{r}
median_age <- median(banksalary$age)

banksalary$age_group <- ifelse(banksalary$age <= median_age, "young", "old")

ggplot(banksalary, aes(x = exper, y = bsal, color = age_group)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Relationship between Experience, Beginning Salary, and Age Group",
       x = "Experience",
       y = "Beginning Salary") +
  scale_color_manual(values = c("young" = "blue", "old" = "orange")) 


```

- Based on the plot, there appears to be two very different slopes between experience and beginning salary when accounting for age group. The young group has a strong positive correlation, while the old group has a weak positive correlation. Due to this large and unexpected difference, I do believe an interaction effect is present between experience and age group. 

b. Build a model for beginning salary that uses experience, age group (young or old), and the interaction between experience and age group. Then, interpret the partial t-test associated with the interaction.

```{r}
interaction_model <- lm(bsal ~ exper + age_group + exper *age_group, data = banksalary)
summary(interaction_model)
```

1. Hypotheses
    - Null Hypothesis: $H_0: \beta_3 = 0$ Implied Model: yhat = beta_0 + beta_1_xi1 + beta_2_xi2
    - Alternative Hypothesis: $H_1: \beta_3 \ne 0$ Implied Model: yhat = beta_0 + beta_1_xi1 + beta_2_xi2 + beta_3_xi3

2. Test Statistic: $t=9.311$

3. p-value: $p = 1.78 \times e^{-5}$

4. Decision: Reject $H_0$. Since $p < .05$

5. Conclusion: There is sufficient evidence to reject that the model with only betas 0,1, and 2 is sufficient. We should use the model that includes the beta3(interaction term) as well.

c. Based on the partial t-test for the age group indicator in the model from Part b., explain why age group should NOT be removed from the model if the model already has experience and the experience-age group interaction even though the p-value suggests a lack of statistical significance.

Even though the p values of exper and age_group are greater than .05 and suggest a lack of significance, they cannot be removed from the model because based on the partial t-test for the age group indicator, the indicator is statistically significant in the model. If we are choosing to keep the indicator term in the model, then we must keep the two terms that create the interaction as well.

d. Using the model from Part b., perform the calculations listed below. You may either perform the calculations by hand or use R to perform them. Either way, include enough work/code so that it is obvious how you are calculating the answers.

Beginning Salary = 5222.183 + (1.122 * x) - 399.958(young/old) + (14.605 * x)

    - Predict the beginning salary for a young employee with 75 months of experience.
  Beginning Salary = 5222.183 + 1.122 * 75 - 399.958 + 14.605 * 75
  = 6001.75
    - Predict the beginning salary for a young employee with 76 months of experience.
  Beginning Salary = 5222.183 + 1.122 * 76 - 399.958 + 14.605 * 76
  = 6017.477
    - Predict the beginning salary for an old employee with 75 months of experience.
  Beginning Salary = 5222.183 + 1.122 * 75 - 0 + 14.605 * 75
  = 6401.708
    - Predict the beginning salary for an old employee with 76 months of experience.
  Beginning Salary = 5222.183 + 1.122 * 76 - 0 + 14.605 * 76
  = 6417.435

e. What impact does an increase of 1 month of experience have on beginning salary? Explain.

One month of experience has a 15.727 (1.122 + 14.605) dollar difference in beginning salary. This means that for every month more of experience that I have, my beginning salary will increase by 15.727 or 15.73 dollars

2. (Comparing Models using Likelihoods) Recall the [gender discrimination study](https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#guided-exercises) and data from HW 1:

```{r, indent = "     "}
banksalary = read.csv(file='https://raw.githubusercontent.com/proback/BeyondMLR/master/data/banksalary.csv', header=T)
```

     Treating beginning salary as the response ($Y$) and denoting predictors experience, education, seniority, and sex by $x_1,\ldots,x_4$, respectively, consider the regression model shown below.
$$\text{Model 1: }Y_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+\beta_3x_{3i}+\beta_4x_{4i}+\epsilon_i$$

     This model can be fit with the following code:

```{r,indent = "      "}
model1 = lm(bsal~exper+educ+senior+sex,data=banksalary)

model2 = lm(bsal~exper+sex,data=banksalary)
```

a. Suppose we want to know if the model above could be reduced to 
\[\text{Model 2: }Y_i=\beta_0+\beta_1x_{1i}+\beta_4x_{4i}+\epsilon_i
\]
State the hypotheses of interest in two ways: 1) in terms of the $\beta$ parameters and 2) in terms of the models (full vs reduced). 

Hypotheses
    - Null Hypothesis: $H_0: \beta_2 = \beta_3 = 0$ 
    - Alternative Hypothesis: $H_1: \beta_j \ne 0 \text{ for at least 1 } j \in \{2,3\}$ 

Models
    - Reduced: $Y_i=\beta_0+\beta_1x_{1i}+\beta_4x_{4i}+\epsilon_i$
    - Full: $Y_i=\beta_0+\beta_1x_{1i}+\beta_2x_{2i}+\beta_3x_{3i}+\beta_4x_{4i}+\epsilon_i$
    
b. Use R's anova() function with both models above to carry out the ANOVA F-test of the desired hypotheses. Be sure to state the value of the test statistic, the p-value, and the conclusion in the context of the problem.

```{r}
anova(model2, model1)
```

1. Hypotheses
    - Null Hypothesis: $H_0: \beta_2 = \beta_3 = 0$ Reduced
    - Alternative Hypothesis: $H_1: \beta_j \ne 0 \text{ for at least 1 } j \in \{2,3\}$ Full

2. Test Statistic: $F=16.454$

3. p-value: $8.498e^-07$

4. Decision: Depends on $p <0.05$, so we reject $H_0$. 

5. Sampling Distribution: F ~ F2,88

6. Conclusion: There is enough evidence to reject that $\beta_2 = \beta_3 = 0$. In other words, there is enough evidence to reject that the the reduced model, which contains only the experience and sex of employees is sufficient when predicting beginning salary.

c. (Manual verification of the results in b.) Note that in the anova results in R, the degrees of freedom for the error (Df.res) is the sample size minus the number of $\beta$ parameters, including the intercept but NOT the error variance $\sigma^2$). So, models with more parameters will have a lower Df.res value reported this way. Verify the anova F-test test statistic by constructing the F statistic manually (i.e., use R as a calculator and calculate the value based on the numbers shown in the anova output). Remember that R refers to Sum of Squares for Error (SSE) as Residual Sum of Squares (RSS).
\[F={(SSE_{\mbox{reduced}}-SSE_{\mbox{full}})/(dfe_\mbox{reduced} - dfe_{\mbox{full}})\over MSE_{\mbox{full}}}
\]

F = ((31130187 - 22657469)/(90 - 88))/(22657469 / 88)
  = about 16.454
  
The manual calculation is nearly exactly equal to the value given in the anova table for the F statistic
  
d. An alternative approach to F-test for this situation is the likelihood-ratio test (LRT), which can be constructed from the statistic
\[G^2=-2(\log L_0-\log L_1)
\]
where $\log L_0$ and $\log L_1$ are the computed (max) log-likelihoods for the reduced and full models, respectively. For large $n$, $G^2$ has an approximate $\chi^2$ distribution with degrees of freedom equal to the difference in parameters between the full and reduced models (same as the numerator Df in the F statistic). Find the LRT test statistic above and its p-value from the approximate $\chi^2$ distribution. State a conclusion in the context of the problem.

```{r}
#Compute LRT Test stat
LRT_stat <- -2*(as.numeric(logLik(model2))-as.numeric(logLik(model1)))
LRT_stat

#Compute LRT p-value
1 - pchisq(q=LRT_stat,df=2)
```
LRT = 29.54503
P = 3.840425e^-07

Decision: $p <.05$ so reject H0

Conclusion: We do have enough evidence to reject the null hypothesis that the reduced model that includes only experience and sex is sufficient when trying to predict beginning salary.

e. How do the results of the LRT compare with those of the F test above?

The results of the LRT are very comparable to the results of the F test above. They have similar p values that are both less than .05, meaning they both allow us to reject the null hypothesis. They both show results suggesting that the reduced model is not sufficient in predicting the beginning salary.

f. The AIC for a given model is given by
\[\mbox{AIC} = -2\log L_1+2(\#\mbox{parameters}),
\]
where the number of parameters here includes the intercept $\beta_0$ AND the error variance $\sigma^2$. Why would a smaller value for AIC be preferred to a larger one?

A smaller AIC is preferred to a larger one because of the -2 in the equation, this means that our goal is to minimize, or get the smallest number possible.

g. Use AIC values to compare the model with all four predictors (model1) to the model with age, sex, and experience (model3). 

```{r}
model3 = lm(bsal~exper+age+sex,data=banksalary)

AIC(model1) 
AIC(model3) 
```
Model1 appears to have a smaller AIC and should be preferred over Model3, but the difference between the AICs is only about 25, which is not a huge difference.

AIC for model1 = 1429.439
AIC for model3 =1455.688

h. Explain why you cannot use the LRT or F-test to compare model1 and model3.

I cannot use the Likelihood Ratio Test or F-test to directly compare Model 1 and Model 3 because these models have two different sets of predictors. The LRT and F-test are used for comparing nested models, where one model is a specific case of the other with some parameters restricted or removed. In this case, Model 1 and Model 3 are not nested within each other. They have different sets of predictors (exper, educ, sex, and senior vs. age, sex, and exper).

## Uploads
Please upload your .html and .Rmd files in Canvas. Name the files using LastnameFirstinitial_HW2.fileextension (i.e., I would use SlifkoM_HW2.html).
