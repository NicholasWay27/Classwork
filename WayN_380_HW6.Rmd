---
title: "STAT380_HW6"
author: Nicholas Way
output: html_document
date: "Due: Monday, October 23, 2023 by 11:59 PM"
---

## Instructions

In class, we covered the basics of using k-fold cross validation for multiple linear regression models and kNN regression models. The activities that follow are meant to provide further practice with these concepts. NOTE: All required `library` commands **must be** included in the Front Matter section. If you include your `library` commands elsewhere in your code, you will be penalized.

At the conclusion to the activity, you should upload

1. your .html file named LastnameFirstInitial_STAT380_HW6.html 
2. your .Rmd file named LastnameFirstInitial_STAT380_HW6.Rmd

## Learning Objectives
This assignment address aspects of the following learning objectives.

1. Students will be able to load datasets from R packages and external sources into the R environment.
2. Students will be able to identify R functions for and perform data wrangling tasks such as filtering observations based on a criterion, creating new variables, and restructuring data.
3. Students will be able to use iteration (loops, apply, etc.) to perform tasks that must be repeated many times on a given dataset.
4. Given a statistical learning method, students will be able to restructure the data for use in the corresponding R function.
5. Fit a linear regression model using statistical software, including situations involving polynomial terms, categorical predictions, and/or interaction terms.
6. Perform k-nearest neighbors regression using statistical software, including using cross validation to select the number of neighbors.
7. Compare the ability of competing models to generalize for new data.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Front Matter - Clean Environment, Load Libraries, User Defined Functions
```{r}
rm(list = ls())

library(tidyverse)
library(FNN) #Use FNN for knn regression

Ins <- read.csv("C:/Users/nicho/OneDrive - The Pennsylvania State University/Stat 380/Lectures/L02_Insurance_m.csv")
Ins <- na.omit(Ins)
```


## Dataset

`L02_Insurance_m.csv` contains information about a number of health insurance policies. In particular, the data set contains some attributes of the policy holder (such as age, sex, etc.) and the total charges billed by the health care provider. Details about the variables included follow.

* `age` - age of primary beneficiary
* `sex` - sex of primary beneficiary
* `bmi` - Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m \^ 2) using the ratio of height to weight, ideally 18.5 to 24.9
* `children` - Number of children covered by the health insurance policy (i.e., the number of dependents)
* `smoker` - Status indicating whether the person is a smoker (options include 'yes' and 'no')
* `region` - the beneficiary's residential area in the US (options include northeast, southeast, southwest, northwest).
* `charges` - Individual medical costs as billed by health insurance

Our goal is to predict `charges` using `age`, `sex`, `bmi`, `children`, and smoking status (`smoker`).

## Problem 1 - Which method (multiple linear regression or kNN) produces the most accurate predictions for new data? To answer this, compare the performance of the methods using 10-fold cross validation.

a. Prepare the data for implementing multiple linear regression and kNN. NOTE: I am not telling you the exact steps to take prepare the data, but if you unsure of the steps, see Problem 2 in HW5. 

```{r}
Ins <- 
  Ins %>%
  mutate(smokes = ifelse(smoker == "yes", 1, 0),
         male = ifelse(sex == "male", 1, 0))


xvars <- c("age", "male", "bmi", "children", "smokes")
Ins[ , xvars] <- scale(Ins[ , xvars], center = TRUE, scale = TRUE)
```


b. Assign the fold values and randomly permute them using a seed of 123. To make sure you are on the right path, the folds for the first few observations are provided below. (If you cannot see the results, check the .html file containing the knitted instructions.)

```{r}
num_folds <- 10
folds <- cut(x = 1:nrow(Ins), breaks = num_folds, labels = FALSE)

set.seed(123)
folds <- sample(folds) #randomly permute the fold values
set.seed(NULL)

folds
```


c. Implement 10-fold cross validation for the multiple linear regression model for predicting `charges` using `age`, `sex`, `bmi`, `children`, and smoking status (`smoker`). For each fold, calculate the Mean Square Error (MSE). Display the 10 MSE values **and** report the mean of the 10 MSE values.

    * NOTE: Be sure to compute the MSE for each fold instead of the RMSE for each fold.
    * NOTE: To implement 10-fold cross validation for this problem (and the next problem), you should write a loop for cycling through the folds as discussed in class instead of using a black box function that does everything behind the scenes.

```{r}
mseVec <- rep(NA, num_folds)

#Loop
for(i in 1:num_folds){
  
#Split data - Fold i becomes validation
ValInd <- which(folds == i)
Validation <- Ins[ValInd, ]
Train <- Ins[-ValInd, ]

#Build model on Train
tempModel <- lm(charges ~ age + male + bmi + children + smokes, data = Train)

#Calculate and store MSE on Validation
yhat <- predict(tempModel, newdata = Validation)
mseVec[i] <- mean((Validation$charges - yhat)^2)
}

#View results
mseVec
mean(mseVec)
mlrRMSE <- sqrt(mean(mseVec))
```


d. Implement 10-fold cross validation for the k-Nearest Neighbors (kNN) regression model for predicting `charges` using `age`, `sex`, `bmi`, `children`, and smoking status (`smoker`). For the purposes of selecting k, consider values of k ranging from 1 to 50. Create a plot showing the Mean Mean Square Error (Mean MSE) for each fold as a function of k **and** state which value of k is optimal.

```{r}
#Initialize
maxK <- 50
MSEMatrix <- matrix(NA, nrow = num_folds, ncol = maxK)
RMSEMatrix <- matrix(NA, nrow = num_folds, ncol = maxK)

#Loop
for(i in 1:num_folds){
  
#Split data - Fold i becomes validation
ValInd <- which(folds == i)
Validation <- Ins[ValInd, ]
Train <- Ins[-ValInd, ]

for(j in 1:maxK){
  

knnRes <- knn.reg(train = Train[ , xvars, drop = FALSE],
                   test = Validation[ , xvars, drop = FALSE],
                   y = Train$charges,
                   k = j)
 #MSE
 MSEMatrix[i,j] <- mean((Validation$charges - knnRes$pred)^2)
 #RMSE
 RMSEMatrix[i,j] <- sqrt(mean((Validation$charges - knnRes$pred)^2))
}}

MeanMSE <- apply(MSEMatrix, 2, mean)
MeanRMSE <- apply(RMSEMatrix, 2, mean)

#creatde data frame
tempDF <- data.frame(k = 1:maxK,
                     mse = MeanMSE)
#plot
ggplot(data = tempDF, mapping = aes(x = k, y = mse)) +
         geom_line() +
  geom_point(data = tempDF[which.min(MeanMSE), ], color = 'red')
#determine min
which.min(MeanMSE)
#min value of rmse
MeanMSE[which.min(MeanMSE)]
```

The optimal value for k is 16 with the minimum MSE value of 24443893.

e. Which method (multiple linear regression or kNN) produces the most accurate predictions for new data? **Explain your reasoning** by discussing the RMSE values associated with each method. NOTE: Although we calculated MSE values in Parts c. and d., compare the RMSE values in this question. Be sure to state the RMSE values.

```{r}
mlrRMSE

MeanRMSE
MeanRMSE[which.min(MeanRMSE)]
which.min(MeanRMSE)
```

The RMSE value for the multiple linear regression method is 6075.479, while the RMSE value for the kNN method is 4899.23.

In my opinion, the difference of the two RMSE values(6075.479 - 4899.23 = 1176.249) is large enough to say that the kNN method produces the most accurate predictions for new data in this particular instance. The smaller RMSE value means that the kNN method has closer predictions than the multiple linear regression method.


## Problem 2 - Plot
Recreate the plot shown below. (If you are not seeing the plot, be sure to open the .html file containing the instructions from Canvas.) The plot shows the plot created in Problem 1d (MSE as a function of k), but includes errors bars. For each value of k, you are to draw a **red** line that goes from  (Mean MSE - standard deviation) to (Mean MSE + standard deviation). The Mean MSE is the mean of the MSE values within each fold. The standard deviation in this problem corresponds to the standard deviation of the MSE values within each fold. NOTE: While the error bars appear to have a similar length, the standard deviation is a different number for each value of k. 

```{r}
StdDevMSE <- apply(MSEMatrix, 2, sd)

resultsDF <- data.frame(k = 1:maxK, MeanMSE = MeanMSE, StandardDeviation = StdDevMSE)

ggplot(data = resultsDF, aes(x = k, y = MeanMSE)) +
  geom_line(color = "black") +
  geom_errorbar(aes(ymin = MeanMSE - StandardDeviation, ymax = MeanMSE + StandardDeviation),
                width = 0.2, color = "red") +
  labs(x = "k", y = "Mean MSE") 

```

